{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unsupervised Learning**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Author - Liam Duncan \\\n",
    "File - unsupervised-model.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** \n",
    "---\n",
    "The dataset used in this study is obtained from the Temple University EEG\\\n",
    "Corpus, which contains EEG recordings collected for psychiatric disorder clas sification. \\\n",
    "The dataset includes both normal and abnormal EEG recordings,\\\n",
    "categorized based on clinical evaluations.\\\n",
    "Source: Lopez, S. (2017). Automated Identification of Abnormal EEGs. \n",
    " Temple University. \\\n",
    " https://isip.piconepress.com/projects/nedc/html/tuh_eeg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decisions:**\n",
    "---\n",
    "All values chosen throughout this notebook were chosen based on a smaller subset of the dataset\\\n",
    "500 files were explored ~250 normal and ~250 abnormal. Experiments regarding clustering algorithms,\\\n",
    "the number of principle components, z-score thresholds, cluster initialization techniques, etc.\\\n",
    "Based on the results found in the data exploration, the optimal model was chosen\n",
    "\n",
    "**Model**\n",
    "---\n",
    "Clustering algorithm: Kmeans\\\n",
    "Number of Clusters: 2\\\n",
    "Cluster Initilization: K-means++\\\n",
    "N_init: 100\\\n",
    "Number of Principle Components: 2\\\n",
    "Z-score Threshold: >22 seen as outliers\\\n",
    "Scaling technique: Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import mode\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, accuracy_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "**Update file paths**\n",
    "\n",
    "Code altered from load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define common EEG channels\n",
    "COMMON_CHANNELS = [\n",
    "    'EEG F7-REF', 'EEG A2-REF', 'EEG C3-REF', 'EEG CZ-REF', 'EEG T3-REF',\n",
    "    'EEG P3-REF', 'EEG C4-REF', 'EEG PZ-REF', 'EEG T5-REF', 'EEG A1-REF',\n",
    "    'EEG FP2-REF', 'EEG FP1-REF', 'SUPPR', 'IBI', 'EEG P4-REF', 'EEG FZ-REF',\n",
    "    'EEG T4-REF', 'EEG O1-REF', 'EEG F8-REF', 'BURSTS', 'EEG O2-REF',\n",
    "    'EEG T6-REF', 'EEG F3-REF', 'EEG F4-REF'\n",
    "]\n",
    "\n",
    "# Fixed length for EEG data after processing\n",
    "FIXED_LENGTH = 30000  # Reduce this if memory is an issue\n",
    "TARGET_SAMPLING_RATE = 250  # Reduced for better training efficiency\n",
    "\n",
    "# Function to find the maximum EEG length in the dataset\n",
    "def find_max_length(folder_paths):\n",
    "    \"\"\"Finds the maximum EEG signal length across all files.\"\"\"\n",
    "    max_length = FIXED_LENGTH  # Use a fixed value to prevent memory issues\n",
    "    print(f\"[INFO] Using fixed max length: {max_length} samples per EEG recording\")\n",
    "    return max_length\n",
    "\n",
    "# Function to load and preprocess EEG data\n",
    "def load_eeg(file_path, target_sampling_rate=TARGET_SAMPLING_RATE, fixed_length=FIXED_LENGTH):\n",
    "    \"\"\"Loads EEG data, selects common channels, resamples, and ensures fixed length.\"\"\"\n",
    "    print(f\"[INFO] Loading file: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "        original_sfreq = raw.info['sfreq']\n",
    "\n",
    "        # Resampling for uniform sample rate\n",
    "        if original_sfreq != target_sampling_rate:\n",
    "            raw.resample(target_sampling_rate)\n",
    "            print(f\"  - Resampled from {original_sfreq} Hz to {target_sampling_rate} Hz\")\n",
    "\n",
    "        available_channels = raw.ch_names\n",
    "        selected_indices = [i for i, ch in enumerate(available_channels) if ch in COMMON_CHANNELS]\n",
    "\n",
    "        if len(selected_indices) < len(COMMON_CHANNELS):\n",
    "            print(f\"[WARNING] {file_path} has only {len(selected_indices)} of {len(COMMON_CHANNELS)} common channels.\")\n",
    "\n",
    "        data = raw.get_data()[selected_indices, :]  # Shape: (channels, time)\n",
    "\n",
    "        # Debugging prints\n",
    "        num_samples = data.shape[1]\n",
    "        print(f\"  - EEG Data Shape Before Processing: {data.shape} (Channels, Time)\")\n",
    "        print(f\"  - Number of Samples Before Padding: {num_samples}\")\n",
    "\n",
    "        # Ensure fixed length\n",
    "        if num_samples > fixed_length:\n",
    "            data = data[:, :fixed_length]  # Truncate long signals\n",
    "            print(f\"  - Truncated to: {fixed_length} samples\")\n",
    "        elif num_samples < fixed_length:\n",
    "            pad_width = fixed_length - num_samples\n",
    "            data = np.pad(data, ((0, 0), (0, pad_width)), mode='constant')  # Pad short signals\n",
    "            print(f\"  - Padded to: {fixed_length} samples\")\n",
    "\n",
    "        # Final shape check\n",
    "        print(f\"  - EEG Data Shape After Processing: {data.shape} (Channels, Time)\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# EEG Dataset Class for Unsupervised Learning (No Labels)\n",
    "# Altered to not use tensor as not needed for unsupervised learning\n",
    "class EEGDataset:\n",
    "    def __init__(self, abnormal_folder, normal_folder):\n",
    "        self.file_paths = []\n",
    "\n",
    "        for file in os.listdir(abnormal_folder):\n",
    "            if file.endswith('.edf'):\n",
    "                self.file_paths.append(os.path.join(abnormal_folder, file))\n",
    "\n",
    "        for file in os.listdir(normal_folder):\n",
    "            if file.endswith('.edf'):\n",
    "                self.file_paths.append(os.path.join(normal_folder, file))\n",
    "\n",
    "        self.fixed_length = find_max_length([abnormal_folder, normal_folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_data = load_eeg(self.file_paths[idx], TARGET_SAMPLING_RATE, self.fixed_length)\n",
    "\n",
    "        if eeg_data is None:\n",
    "            print(f\"[ERROR] Skipping file at index {idx} due to loading failure.\")\n",
    "            return None\n",
    "\n",
    "        return eeg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE PATHS \n",
    "train_abnormal_folder = r\"---------------UPDATE PATH -------------------\"\n",
    "train_normal_folder = r\"---------------UPDATE PATH -------------------\"\n",
    "\n",
    "print(\"[INFO] Initializing EEG dataset...\")\n",
    "all_train_dataset = EEGDataset(train_abnormal_folder, train_normal_folder)\n",
    "\n",
    "print(f\"[INFO] Total EEG Files Loaded: {len(all_train_dataset)}\")\n",
    "\n",
    "# Extract EEG data\n",
    "all_train_data = []\n",
    "\n",
    "for idx in range(len(all_train_dataset)):\n",
    "    eeg_data = all_train_dataset[idx]\n",
    "    if eeg_data is not None:\n",
    "        all_train_data.append(eeg_data.flatten())\n",
    "        # Each sample is flattened to allow for clustering as easier with 2D data\n",
    "\n",
    "all_train_data = np.array(all_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This  section is used for getting ground truth labels and for plotting the abnormal data on its own\n",
    "train_abnormal_data = []\n",
    "train_normal_data = []\n",
    "\n",
    "for idx, file_path in enumerate(all_train_dataset.file_paths):\n",
    "    if train_abnormal_folder in file_path:\n",
    "        train_abnormal_data.append(all_train_data[idx])\n",
    "    else:\n",
    "        train_normal_data.append(all_train_data[idx])\n",
    "\n",
    "train_abnormal_data = np.array(train_abnormal_data)\n",
    "train_normal_data = np.array(train_normal_data)\n",
    "\n",
    "train_abnormal_length = (len(train_abnormal_data))\n",
    "train_normal_length = (len(train_normal_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE PATHS\n",
    "test_abnormal_folder = r\"---------------UPDATE PATH -------------------\"\n",
    "test_normal_folder = r\"---------------UPDATE PATH -------------------\"\n",
    "\n",
    "print(\"[INFO] Initializing EEG dataset...\")\n",
    "test_dataset = EEGDataset(test_abnormal_folder, test_normal_folder)\n",
    "\n",
    "print(f\"[INFO] Total EEG Files Loaded: {len(test_dataset)}\")\n",
    "\n",
    "# Extract EEG data\n",
    "test_data = []\n",
    "\n",
    "for idx in range(len(test_dataset)):\n",
    "    eeg_data = test_dataset[idx]\n",
    "    if eeg_data is not None:\n",
    "        test_data.append(eeg_data.flatten())\n",
    "        # Each sample is flattened to allow for clustering as easier with 2D data\n",
    "\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This  section is used for splitting data so ground truth labels of test data can be retrieved\n",
    "test_abnormal_data = []\n",
    "test_normal_data = []\n",
    "\n",
    "for idx, file_path in enumerate(test_dataset.file_paths):\n",
    "    if test_abnormal_folder in file_path:\n",
    "        test_abnormal_data.append(test_data[idx])\n",
    "    else:\n",
    "        test_normal_data.append(test_data[idx])\n",
    "\n",
    "test_abnormal_data = np.array(test_abnormal_data)\n",
    "test_normal_data = np.array(test_normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Outlier Removal, Standard Scaling, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data for better clustering performance\n",
    "print(\"[INFO] Scaling Training Dataset\")\n",
    "scaler = StandardScaler()\n",
    "all_train_data_sclaed = scaler.fit_transform(all_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels (0 = Abnormal, 1 = Normal)\n",
    "labels = np.array([0] * train_abnormal_length + [1] * train_normal_length)\n",
    "colors = np.array([\"red\" if label == 0 else \"blue\" for label in labels])\n",
    "label_names = {0: \"Abnormal\", 1: \"Normal\"}\n",
    "\n",
    "\n",
    "# t-SNE Visualization \n",
    "print(\"[INFO] Performing t-SNE Visualization\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init=\"pca\")\n",
    "X_tsne = tsne.fit_transform(all_train_data_sclaed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for label, color in zip([0, 1], [\"red\", \"blue\"]):\n",
    "    plt.scatter(X_tsne[labels == label, 0], X_tsne[labels == label, 1], \n",
    "                c=color, label=label_names[label], alpha=0.7)\n",
    "plt.title(\"t-SNE Visualization of EEG Training Data\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend(title=\"EEG Type\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Performing KMeans Clustering on Abnormal Data\")\n",
    "abnormal_kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42, n_init=100)\n",
    "final_labels = abnormal_kmeans.fit_predict(train_abnormal_data)\n",
    "pca = PCA(n_components=2)\n",
    "train_abnormal_data_pca = pca.fit_transform(train_abnormal_data)\n",
    "sns.scatterplot(x=train_abnormal_data_pca[:, 0], y=train_abnormal_data_pca[:, 1], hue=final_labels, palette='viridis', legend='full')\n",
    "plt.title(\"K-Means Clustering on Abnormal Training Data\")\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Removing Outliers and Performing PCA on Training Data\")\n",
    "# Compute Z-scores\n",
    "z_scores = np.abs(stats.zscore(all_train_data_sclaed))\n",
    "\n",
    "# Remove outliers based on the current threshold\n",
    "train_data_no_outliers = all_train_data_sclaed[(z_scores < 22).all(axis=1)]\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "train_data_pca = pca.fit_transform(train_data_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Data\n",
    "Do not remove outliers of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Performing Scaling and PCA on Test Data\")\n",
    "# Standardize the data for better clustering performance\n",
    "test_data_scaled = scaler.fit_transform(test_data)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "test_data_pca = pca.fit_transform(test_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Performing KMeans Clustering on Training Data\")\n",
    "kmeans = KMeans(n_clusters=2, random_state=47, n_init=100, init='k-means++')\n",
    "kmeans_labels = kmeans.fit_predict(train_data_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clustering results\n",
    "def plot_clusters(data, labels, title):\n",
    "    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=labels, palette='viridis', legend='full')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ground truth labels\n",
    "ground_truth_labels = np.zeros(len(train_data_pca))  \n",
    "ground_truth_labels[train_abnormal_length:] = 1  \n",
    "\n",
    "# KMeans assigns arbitrary labels (0/1), so we need to map them to the ground truth\n",
    "mapped_labels = np.zeros_like(kmeans_labels)\n",
    "\n",
    "# Find the most common ground truth label for each cluster and assign mapped labels\n",
    "for cluster in np.unique(kmeans_labels):\n",
    "    mask = kmeans_labels == cluster\n",
    "    most_common_label = mode(ground_truth_labels[mask], keepdims=True).mode[0]\n",
    "    mapped_labels[mask] = most_common_label\n",
    "\n",
    "# Compute accuracy\n",
    "correct_assignments = np.sum(mapped_labels == ground_truth_labels)\n",
    "total_points = len(ground_truth_labels)\n",
    "accuracy = (correct_assignments / total_points) * 100\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"[Results] Training Data\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"Correctly clustered points: {accuracy}%\")\n",
    "train_sil_score = silhouette_score(train_data_pca, kmeans_labels)\n",
    "train_dbi_score = davies_bouldin_score(train_data_pca, kmeans_labels)\n",
    "train_ari_score = adjusted_rand_score(ground_truth_labels, kmeans_labels)\n",
    "print(f\"Silhouette Score for Train Data: {train_sil_score} \")\n",
    "print(f\"Davies-Bouldin Index Score for Train Data: {train_dbi_score} \")\n",
    "print(f\"Adjusted Rand Index Score for Train Data: {train_ari_score} \")\n",
    "print(\"Abnormal Data Label: 0\")\n",
    "print(\"Normal Data Label: 1\")\n",
    "plot_clusters(train_data_pca, kmeans_labels, \"K-Means Clustering of All Train Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction and Analysis - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Performing KMeans Clustering on Test Data\")\n",
    "test_cluster_labels = kmeans.predict(test_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_test_size = len(test_abnormal_data)\n",
    "\n",
    "abnormal_segment = test_cluster_labels[:abnormal_test_size]  # Abnormal data\n",
    "normal_segment = test_cluster_labels[abnormal_test_size:]   # Normal data\n",
    "\n",
    "# Find the majority cluster in each segment\n",
    "majority_cluster_1 = Counter(abnormal_segment).most_common(1)[0][0]  # Abnormal data\n",
    "majority_cluster_2 = Counter(normal_segment).most_common(1)[0][0]  # Normal data\n",
    "\n",
    "print(f\"Majority cluster for abnormal data: {majority_cluster_1}\")\n",
    "print(f\"Majority cluster for normal data: {majority_cluster_2}\")\n",
    "\n",
    "# Update cluster-to-label mapping based on majority cluster identification\n",
    "cluster_to_label = {\n",
    "    majority_cluster_1: 0,  # Abnormal: 0\n",
    "    majority_cluster_2: 1   # Normal: 1\n",
    "}\n",
    "\n",
    "# Assign labels to test data\n",
    "test_pred_labels = np.array([cluster_to_label.get(c, 0) for c in test_cluster_labels])\n",
    "\n",
    "# Ground truth (abnormal: 0, normal: 1)\n",
    "y_test = np.array([0] * abnormal_test_size + [1] * (len(test_cluster_labels) - abnormal_test_size))\n",
    "\n",
    "# Compute accuracy\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"[Results] Testing Data\")\n",
    "print(\"------------------------------------------------\")\n",
    "accuracy = accuracy_score(y_test, test_pred_labels)\n",
    "print(f\"Test Clustering Accuracy: {accuracy}\")\n",
    "test_sil_score = silhouette_score(test_data_pca, test_cluster_labels)\n",
    "test_dbi_score = davies_bouldin_score(test_data_pca, test_cluster_labels)\n",
    "test_ari_score = adjusted_rand_score(y_test, test_cluster_labels)\n",
    "print(f\"Silhouette Score for Test Data: {test_sil_score}\")\n",
    "print(f\"Davies-Bouldin Index Score for Test Data: {test_dbi_score} \")\n",
    "print(f\"Adjusted Rand Index Score for Test Data: {test_ari_score} \")\n",
    "plot_clusters(test_data_pca, test_cluster_labels, \"K-Means Clustering of Test Data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
